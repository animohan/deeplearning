{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with a Perceptron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Write a note summarizing the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) #Set default plot sizes\n",
    "plt.rcParams['image.interpolation'] = 'nearest' #Use nearest neighbor for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allowing notebooks to reload external python modules\n",
    "# Details: http://stackoverflow.com/questions/1907993/aut\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "For this simple example, I will use a handcrafted dataset. The dataset is generated from the equation e^(0.3*x1 + 0.6*x2 + 0.5). Values above 3.0 are marked as belonging to class 1 and below 3.0 are marked as belonging to class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: [ 1.03045453  1.06183655  1.09417428  1.12749685  1.16183424  1.19721736]\n",
      "x2: [ 1.43332941  1.52196156  1.6160744   1.71600686  1.8221188   1.93479233]\n",
      "Exponentiated values: [ 2.43512965  2.66445624  2.9153795   3.18993328  3.49034296  3.81904351]\n"
     ]
    }
   ],
   "source": [
    "#here is an artificial dataset\n",
    "x1 = np.array(np.arange(0.1,0.7,0.1))\n",
    "print(\"x1:\", np.exp(x1*0.3))\n",
    "x2 = np.array(np.arange(0.6,1.1,0.1))\n",
    "print(\"x2:\", np.exp(x2*0.6))\n",
    "\n",
    "#e^(W11*x1 + W12*x2 + B1)\n",
    "print(\"Exponentiated values:\", np.exp(0.3*x1 + 0.6*x2 + 0.5))\n",
    "\n",
    "#From the output, lets use 3 as threshold; Value>3 = class 1, value<3 = class 0\n",
    "Y = np.array([0,0,0,1,1,1])\n",
    "X = np.array([x1,x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential values for moderately large numbers tend to overflow. So np.clip is used here to limit the values of the signal between -500 and 500. Since e^x is between 0 and 1, the error in using this clip is low.Additionally, here I am using a numerically stable version of sigmoid function\n",
    "Note that we can write \n",
    "$\\frac{1}{1+e^-z}$ as $\\frac{e^z}{1+e^z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500,500)\n",
    "    if x.any()>=0:\n",
    "        return 1/(1+ np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_parameters(dim1, dim2=1,std=1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1,dim2])*std)\n",
    "    else:\n",
    "        return(np.zeros([dim1,dim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am assuming a single layered network. Note that event with single layered network, the layer itself can have multiple nodes. Also, I am using vectorized operations here i.e not using explicit loops. This helps in processing multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Single layer network: Forward Prop\n",
    "def fwd_prop(W1,B1,X,Y):\n",
    "    n_0 = np.shape(X)[0]\n",
    "    m   = np.shape(X)[1]\n",
    "    n_1 = np.shape(W1)[0]     \n",
    "    \n",
    "    #Dimensions\n",
    "    # X = (n_0,m)\n",
    "    # W1 = (n_1,n_0)\n",
    "    # B1 = (n_1,1) -> Broadcast -> (n_1,m)\n",
    "    # Z1 = (n_1,m)\n",
    "    # A1 = (n_1,m)\n",
    "    Z1 = np.dot(W1,X) + B1#\n",
    "    A1 = sigmoid(Z1)\n",
    "    #print(\"A1 shape\", np.shape(A1))\n",
    "    return(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am calculating the loss/cost. The loss function here is a logistic loss function and in this case of binary classification, this is also a cross-entropy loss\n",
    "\n",
    "Cross Entropy loss for a single datapoint = $\\sum_{i=1}^{c} y_i*log (\\hat y_i) $\n",
    "For binary classification: $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i) $\n",
    "\n",
    "Lastly, the gradients w.r.t W1 and B1 are calculated and returned along with the total cost/loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Derive Backprop equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Single layer network: Backprop\n",
    "\n",
    "def back_prop(A1,W1,B1,X,Y):\n",
    "    n_0 = np.shape(X)[0]\n",
    "    m   = np.shape(X)[1]\n",
    "    n_1 = np.shape(W1)[0]\n",
    "    \n",
    "    \n",
    "    cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1))\n",
    "    #shape(dA1) = shape(A1) = (n_1,m)\n",
    "    dA1 = -Y/A1 + (1-Y)/(1-A1)\n",
    "    \n",
    "    #shape(dZ1) = shape(Z1) = (n_1,m)\n",
    "    dZ1 = (1/m)*(A1 - Y)\n",
    "    \n",
    "    #shape(dW1) = shape(W1) = (n_1,n_0)\n",
    "    dW1 = np.dot(dZ1, X.T) # dZ1 = (n_1,m), X ->(n_0,m)\n",
    "    \n",
    "    #shape(dB1) = shape(B1) = (n_1,1)\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads ={\"dW1\": dW1, \"dB1\":dB1}\n",
    "    \n",
    "    return(grads,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs a simple gradient descent. After completing a round of forward propagation and backward propagation, the weights are updated based on the learning rate and gradient. The loss for that iteration is recorded in the loss_array. The function returns the final parameters W1,B1 and the loss array after running for given number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_grad_desc(num_iterations, learning_rate,X,Y,n_1):\n",
    "    n_0, m = np.shape(X)\n",
    "    \n",
    "    W1 = init_parameters(n_1, n_0, random= False)\n",
    "    B1 = init_parameters(n_1,1, random = False)\n",
    "    \n",
    "    loss_array = np.ones([num_iterations])*np.nan\n",
    "    for i in np.arange(num_iterations):\n",
    "        A1 = fwd_prop(W1,B1,X,Y)\n",
    "        grads,cost = back_prop(A1,W1,B1,X,Y)\n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]\n",
    "        \n",
    "        loss_array[i] = cost\n",
    "        \n",
    "        parameter = {\"W1\":W1,\"B1\":B1}\n",
    "    \n",
    "    return(parameter,loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "All the helper functions have been created and now I am just running gradient descent on the handcrafted dataset that I had created earlier. Note that I am using n_1 = 1, therefore, I am just using one node. This is equivalent to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B1': array([[ 3.44835516]]), 'W1': array([[-3.75759556, -3.88372978]])}\n"
     ]
    }
   ],
   "source": [
    "params, loss_array = run_grad_desc(100000,0.001,X,Y,n_1= 1 )\n",
    "#print(loss_array)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataset is generated from the equation e^(0.3*x1 + 0.6*x2 + 0.5). Values above 3.0 are marked as belonging to class 1 and below 3.0 are marked as belonging to class 0. The values generated after running the algorithm do not necessarily match to the weights we used in generating the data i.e 0.3, 0.6 and bias of 0.5. However, if we look multiple the weights with the input data we get the following output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.64098917,   3.52132258,   3.68914767,  13.95566493,\n",
       "        13.95566493,  15.23259494])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]*params[\"W1\"][0][0]* X[1]*params[\"W1\"][0][1] + params[\"B1\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there is  boundary of separation between 1st 3 values and last 3 values. \n",
    "\n",
    "Key thing to note here is that the data we generated was a linearly separable data and hence there are many possible options for the separting plane. Unlike SVM, logistic regression does not necessarily find the best separting plane, but finds a locally optimum solution that separates the classes of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the loss array\n",
    "Here we want to ensure that the loss value per iteration is going down. However, note that the plot has not curved to reach stablizing value i.e we can run the algorithms more times to get a lower loss. However, this is not needed as the current value of the parameters can classify the input data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f19601aaba8>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAHVCAYAAAAzabX0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW7ibD1G6tQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f196377e828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with handcrafted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00063367,  0.00062571,  0.00063688,  0.00132048,  0.00132048,\n",
       "        0.00140551])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0.25,0.75],[0.1,0.9],[0.3,0.8],[0.8,0.25],[0.9,0.2],[0.7,0.1]])\n",
    "X = np.array([[0.11,0.12],[0.05,0.1],[0.15,0.11],[0.8,0.9],[0.9,0.8],[0.85,0.95]])\n",
    "X = X.T #Had to do this because, I did not declare the X array as (#dimension * # Datapoints)\n",
    "Y = np.array([1,1,1,0,0,0])\n",
    "params, loss_array = run_grad_desc(100,0.01,X,Y,n_1= 1 )\n",
    "#print(loss_array)\n",
    "X[0]*params[\"W1\"][0][0]* X[1]*params[\"W1\"][0][1] + params[\"B1\"][0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
