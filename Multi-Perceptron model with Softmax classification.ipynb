{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Perceptron model with Softmax classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am building a multi-perceptron model i.e a neural network with 1 hidden layer. However in the output layer, instead of sigmoid unit, I am going to build a softmax unit that provides the relative probabilities of the various classes out output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Put up a picture of a multi-perceptron model and explain what this two layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) #Set default plot sizes\n",
    "plt.rcParams['image.interpolation'] = 'nearest' #Use nearest neighbor for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Allowing notebooks to reload external python modules\n",
    "# Details: http://stackoverflow.com/questions/1907993/aut\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Use pickle to unpack to cifar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    data = pickle.load(fo, encoding = 'bytes')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = 'cifar/data_batch_1'\n",
    "u = unpickle(file)\n",
    "\n",
    "x = u[b'data'].reshape(10000,3,32,32).transpose(0,2,3,1).astype('float')\n",
    "y = np.array(u[b'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = {\"plane\":0,'car':1,'bird':2,'cat':3,'deer':4,'dog':5,'frog':6,'horse':7,'ship':9,'truck':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_cifar_samples(x,y,class_of_interest):\n",
    "        number_of_samples = 7\n",
    "        idclass = classes[class_of_interest]\n",
    "        y_sample = np.where(y == idclass)\n",
    "        \n",
    "        y_idx = y_sample[0][1:number_of_samples]\n",
    "        offset = 1\n",
    "        plt.figure(1)\n",
    "\n",
    "        for i in y_idx:\n",
    "\n",
    "            plt.subplot(number_of_samples,number_of_samples,offset)\n",
    "            plt.imshow(x[i].astype('uint8'))\n",
    "            if(offset == 1):\n",
    "                plt.title(class_of_interest)\n",
    "            offset= offset + 1\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        y_otherclass = np.where(y!=idclass)\n",
    "        y_idx = y_otherclass[0][1:number_of_samples]\n",
    "\n",
    "        offset = 1\n",
    "        plt.figure(2)\n",
    "        \n",
    "        for i in y_idx:\n",
    "            plt.subplot(number_of_samples,number_of_samples,offset)\n",
    "            plt.imshow(x[i].astype('uint8'))\n",
    "            if(offset == 1):\n",
    "                plt.title(\"Other Class\")\n",
    "            offset= offset + 1\n",
    "            plt.axis('off')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "        #Normalizing the vector\n",
    "    mean_x = np.mean(x)\n",
    "    stddev_x = np.std(x)\n",
    "    x_output  = (x- mean_x) / stddev_x \n",
    "    \n",
    "    return(x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_cifar_train_test_set(x, y, classes, show_samples = 0):\n",
    "\n",
    "       \n",
    "    if show_samples:\n",
    "        show_cifar_samples(x,y,class_of_interest)\n",
    "\n",
    "        \n",
    "    x_dataset = x\n",
    "    y_dataset = np.zeros([len(classes.keys()),len(y)])\n",
    "    for datapoint in np.arange(len(y)):\n",
    "        class_of_datapoint = y[datapoint] #For the kth datapoint y[k] will give the object class\n",
    "        y_dataset[class_of_datapoint,datapoint] = 1 # The object class the kth datapoint belongs to is set to 1;\n",
    "                                                    # the column index- datapoint, provides the datapoint under consideration\n",
    "        \n",
    "    #Resulting y_dataset is of dimension - (# of classes, number of datapoints), so before using train_test_split,\n",
    "    # we will transform it. We will tranform it back to this dimensional form later.\n",
    "    y_dataset = y_dataset.T\n",
    "\n",
    "    \n",
    "    xtrain, xtest, y_train, y_test = train_test_split(x_dataset, y_dataset, test_size = 0.2)\n",
    "    x_train_vector = xtrain.reshape(xtrain.shape[0],-1).T\n",
    "    x_test_vector = xtest.reshape(xtest.shape[0],-1).T\n",
    "    \n",
    "    #Setting the dimensions of output vector to be (# of classes, number of datapoints),\n",
    "    y_train = y_train.T\n",
    "    y_test  = y_test.T\n",
    "    \n",
    "    x_train = normalize(x_train_vector)\n",
    "        \n",
    "    #For normalizing test data, we will use the mean and std.dev from the training data (i.e not the mean and std.dev\n",
    "    # from the test data)\n",
    "    mean_x = np.mean(x_train_vector)\n",
    "    stddev_x = np.std(x_train_vector)\n",
    "    x_test = (x_test_vector - mean_x)/stddev_x\n",
    "        \n",
    "        \n",
    "    return(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = generate_cifar_train_test_set(x, y, classes, show_samples = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3072, 8000)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential values for moderately large numbers tend to overflow. So np.clip is used here to limit the values of the signal between -500 and 500. Since e^x is between 0 and 1, the error in using this clip is low.Additionally, here I am using a numerically stable version of sigmoid function\n",
    "Note that we can write \n",
    "$\\frac{1}{1+e^-z}$ as $\\frac{e^z}{1+e^z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500,500)\n",
    "    if x.any()>=0:\n",
    "        return 1/(1+ np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0] = 0\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    #for stabilization of softmax, we will calculate and subtract the max value*\n",
    "    # TO DO: explain this.\n",
    "    stabilize_factor = -1*np.max(x, axis = 0, keepdims = True)\n",
    "    \n",
    "    unnormalized_probabilities = np.exp(x+stabilize_factor)\n",
    "        \n",
    "    normalization_factor = np.sum(unnormalized_probabilities,axis = 0, keepdims = True)\n",
    "    \n",
    "    #if(normalization_factor == 0):\n",
    "    #    normalized_probabilities = unnormalized_probabilities\n",
    "    #else:\n",
    "    normalized_probabilities = unnormalized_probabilities/normalization_factor\n",
    "    \n",
    "    return(normalized_probabilities)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_parameters(dim1, dim2,std=1e-1, random = True):\n",
    "\n",
    "    if(random):\n",
    "        return(np.random.random([dim1,dim2])*std)\n",
    "    else:\n",
    "        return(np.zeros([dim1,dim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am assuming a single layered network. Note that event with single layered network, the layer itself can have multiple nodes. Also, I am using vectorized operations here i.e not using explicit loops. This helps in processing multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Two layer network: Forward Prop\n",
    "def fwd_prop(W1,B1,W2,B2,X,Y):\n",
    "    n_0 = np.shape(X)[0]\n",
    "    m   = np.shape(X)[1]\n",
    "    n_1 = np.shape(W1)[0]\n",
    "    n_2 = np.shape(W2)[0]\n",
    "    #print(\"n_0:\",n_0)\n",
    "    #print(\"m:\",m)\n",
    "    #print(\"n_1\",n_1)\n",
    "    \n",
    "    #Dimensions\n",
    "    # X = (n_0,m)\n",
    "    # W1 = (n_1,n_0)\n",
    "    # B1 = (n_1,1) -> Broadcast -> (n_1,m)\n",
    "    # Z1 = (n_1,m)\n",
    "    # A1 = (n_1,m)\n",
    "    \n",
    "\n",
    "    Z1 = np.dot(W1,X) + B1\n",
    "    #A1 = sigmoid(Z1)\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2,A1) + B2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "   \n",
    "    return(A2,A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on derivation of dZ2 from cost\n",
    "\n",
    "$cost = -(1/m)*\\sum{y_i*log(\\hat y_i)}$\n",
    "\n",
    "Now,\n",
    "$\\hat y_j = \\frac{e^{z_j}}{\\sum_i{e^{z_i}}}$\n",
    "\n",
    "Let us say $y_2$ is the correct class. We need to calculate $\\frac{\\partial c}{ \\partial y_2}$ and $\\frac{\\partial c}{ \\partial y_i}$ where $i \\neq 2$\n",
    "\n",
    "1) First let us calculate:\n",
    "\n",
    "$\\frac{\\partial c}{ \\partial z_2} = \\frac{-1}{m} * \\frac{\\partial }{ \\partial z_2} \\sum{y_i*log(\\hat y_i)}$\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\partial}{ \\partial z_2} (y_1*log(\\hat y_1) + y_2*log(\\hat y_2) + ...)$\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\partial}{\\partial z_2}y_1*log(\\frac {e^{z_1}}{\\sum_i{e^{z_i}}}) + \\frac{\\partial }{\\partial z_2}y_2*log(\\frac {e^{z_2}}{\\sum_i{e^{z_i}}}) + ...$\n",
    "\n",
    "$ y_2 = 1, y_i = 0$ for $ {i\\neq2} $ Because true class has value 1 and rest are 0 in the given y output\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\partial }{\\partial z_2}log(\\frac {e^{z_2}}{\\sum_i{e^{z_i}}})$\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\sum_i{e^{z_i}}}{e^{z_2}} * \\frac{\\partial }{\\partial z_2} \\frac {e^{z_2}}{\\sum_i{e^{z_i}}}$\n",
    "\n",
    "$ = \\frac{-1}{m} *\\frac{\\sum_i{e^{z_i}}}{e^{z_2}}*\\frac{\\partial }{\\partial z_2} \\frac {e^{z_2}}{e^{z_1} + e^{z_2} + e^{z_3} + ..}$\n",
    "\n",
    "$ =  \\frac{-1}{m} *\\frac{\\sum_i{e^{z_i}}}{e^{z_2}}* \\frac{ \\sum_i {e^{z_i}}*\\frac{\\partial }{\\partial z_2} e^{z_2} - e^{z_2}*\\frac{\\partial }{\\partial z_2} \\sum_i{e^{z_i}}  }{(\\sum_i{e^{z_i}})^2}$\n",
    "\n",
    "$ =  \\frac{-1}{m} *\\frac{\\sum_i{e^{z_i}}}{e^{z_2}}* \\frac{ \\sum_i {e^{z_i}}*e^{z_2} - e^{z_2}*e^{z_2}  }{(\\sum_i{e^{z_i}})^2}$\n",
    "\n",
    "$ =  \\frac{-1}{m} *\\frac{ \\sum_i {e^{z_i}} - e^{z_2}}{\\sum_i{e^{z_i}}}$\n",
    "\n",
    "$ =  \\frac{-1}{m} *(1 - \\frac{e^{z_2}}{{\\sum_i{e^{z_i}}}})$\n",
    "\n",
    "$ =  \\frac{1}{m} *(\\frac{e^{z_2}}{{\\sum_i{e^{z_i}}}} - 1)$\n",
    "\n",
    "$ =  \\frac{1}{m} *(y_2 - 1)$\n",
    "\n",
    "$\\frac{\\partial c}{ \\partial z_2} = \\frac{1}{m} *(y_2 - 1)$  ..... (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Next let us calculate $\\frac{\\partial c}{ \\partial z_i}$ which can be true for any $i \\neq2$. Here we will take i = 1\n",
    "\n",
    "$\\frac{\\partial c}{ \\partial z_1} = \\frac{-1}{m} * \\frac{\\partial }{ \\partial z_1} \\sum{y_i*log(\\hat y_i)}$\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\partial}{ \\partial z_1} (y_1*log(\\hat y_1) + y_2*log(\\hat y_2) + ...)$\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\partial}{\\partial z_1}y_1*log(\\frac {e^{z_1}}{\\sum_i{e^{z_i}}}) + \\frac{\\partial }{\\partial z_2}y_2*log(\\frac {e^{z_1}}{\\sum_i{e^{z_i}}}) + ...$\n",
    "\n",
    "$ y_2 = 1, y_i = 0$ for $ {i\\neq2} $ Because true class has value 1 and rest are 0 in the given y output\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\partial }{\\partial z_1}log(\\frac {e^{z_2}}{\\sum_i{e^{z_i}}})$\n",
    "\n",
    "$ = \\frac{-1}{m} * \\frac{\\sum_i{e^{z_i}}}{e^{z_2}} * \\frac{\\partial }{\\partial z_1} \\frac {e^{z_2}}{\\sum_i{e^{z_i}}}$\n",
    "\n",
    "$ = \\frac{-1}{m} *\\frac{\\sum_i{e^{z_i}}}{e^{z_2}}*\\frac{\\partial }{\\partial z_1} \\frac {e^{z_2}}{e^{z_1} + e^{z_2} + e^{z_3} + ..}$\n",
    "\n",
    "$ =  \\frac{-1}{m} *\\frac{\\sum_i{e^{z_i}}}{e^{z_2}}* \\frac{ \\sum_i {e^{z_i}}*\\frac{\\partial }{\\partial z_1} e^{z_2} - e^{z_2}*\\frac{\\partial }{\\partial z_1} \\sum_i{e^{z_i}}  }{(\\sum_i{e^{z_i}})^2}$\n",
    "\n",
    "$ =  \\frac{-1}{m} *\\frac{\\sum_i{e^{z_i}}}{e^{z_2}}* \\frac{ \\sum_i {e^{z_i}}*0 - e^{z_2}*e^{z_1}  }{(\\sum_i{e^{z_i}})^2}$\n",
    "\n",
    "$ =  \\frac{1}{m} *\\frac{e_{z_1}}{\\sum_i{e^{z_i}}}$\n",
    "\n",
    "$ =  \\frac{1}{m} * y_1$\n",
    "\n",
    "\n",
    "$\\frac{\\partial c}{ \\partial z_1} = \\frac{1}{m} * y_1$  ..... (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence from equation (1) and (2) we get that the for the correct class the back prop is softmax value - 1 and for incorrect class the backprop value is softmax value itself\n",
    "Hence in the upcoming back prop we can capture this as\n",
    "\n",
    "$dZ2 = \\frac{1}{m}*(A2 - Y)$ where A2 is the output of the softmax function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def back_prop(A1,W1,B1,A2,W2,B2,X,Y):\n",
    "    n_0 = np.shape(X)[0]\n",
    "    m   = np.shape(X)[1]\n",
    "    n_1 = np.shape(W1)[0]\n",
    "    n_2 = np.shape(W2)[0] \n",
    "      \n",
    "    cost = (-1/m)*np.sum(Y*np.log(A2) + (1-Y)*np.log(1-A2)) + 0.5*1e-5*np.sum(W1*W1) + 0.5*1e-5*np.sum(W2*W2)\n",
    "       \n",
    "    #A2 is the output of the softmax function of Z2 i.e A2 = e^Z2/Sum(e^Z2)\n",
    "    # It is easier to just derive dcost/dZ2 directly\n",
    "    # The explanation of derivation of dZ2 is given above\n",
    "    \n",
    "    dZ2 = (1/m)*(A2 - Y)    \n",
    "\n",
    "    #shape(dW2) = shape(W2) = (n_2,n_1)\n",
    "    #Note that (1/m) factor is taken care of in the calculation of dZ2\n",
    "    \n",
    "    dW2 = np.dot(dZ2, A1.T) # (dZ2) = (n_2,m), A1 ->(n_1,m)\n",
    "                                    # Hence result shape = (n_2,n_1) = (1,n_1)\n",
    "    \n",
    "    # Z2 = np.dot(W2,A1) + B2\n",
    "    # dB2 = dZ2*1\n",
    "    # shape(dB2) = shape(B2) = (n_2,1) = ; (dZ2) = (n_2,m) hence to get (n_2,1) we sum across all columns\n",
    "    dB2 = np.sum(dZ2, axis = 1, keepdims = True) \n",
    "    \n",
    "        \n",
    "    # Z2 = np.dot(W2,A1) + B2\n",
    "    # dA1 = dZ2*W2 (consider the dimensions)\n",
    "    #(A1) = (n_1,m); (dZ2) = (n_2,m), (W2) = (n_2,n_1)\n",
    "    # Hence dot product of W2.T and dZ2 gives correct output dimenstion for dA1\n",
    "    dA1 = np.dot(W2.T,dZ2) \n",
    "    \n",
    "    ###--------Sigmoid Backprop-------##\n",
    "    # A1 = sigmoid(Z1)\n",
    "    # (A1) = (n_1,m)\n",
    "    # (Z1) = (n_1,m)\n",
    "    #dZ1 = dA1*(A1)*(1-A1) #(dA1) = (n_1,m), (A1) = (n_1,m), hence elementwise product for preserving dimensions\n",
    "    ##--------------------------------##\n",
    "    \n",
    "    ##---------- RELU Backprop---------##\n",
    "    # A1 = Relu(Z1) i.e Z1 = Z1 if Z1>0 else 0\n",
    "    # hence when derivative A1 w.r.t Z1 = 1 if Z1>0 else 0\n",
    "    \n",
    "    # NOTE: Uncomment following 4 lines for relu and comment the above dZ1 = dA1*(A1)*(1-A1)\n",
    "    Z1 = np.dot(W1,X) + B1\n",
    "    dA1dZ1 = np.zeros(np.shape(Z1))\n",
    "    dA1dZ1[Z1>0] = 1\n",
    "    dZ1 = dA1*dA1dZ1\n",
    "\n",
    "    ##---------------------------------##\n",
    "    \n",
    "    # Z1 = np.dot(W1,X) + B1\n",
    "    # dW1 = dZ1*X\n",
    "    # (dZ1) = (n_1,m); (X) = n_0,m); (W1) = (n_1,n_0). hence (dW1) = (n_1,n_0)\n",
    "    # product of dZ1* X.T gives the correct putput dimension for X\n",
    "    dW1 = np.dot(dZ1, X.T) #(dZ1) = (n_1,m); (X) = (n_0,m) hence dot product(dZ1,X.T) = (n_1,n_0)\n",
    "    \n",
    "    # Z1 = np.dot(W1,X) + B1\n",
    "    # dB1 = dZ1*1\n",
    "    #(dZ1) = (n_1,m); (B1) = (n_1,1) hence (db1 = (n_1,1)\n",
    "    # dB1 = np.sum (dZ1) along all columns to get output dimension\n",
    "    \n",
    "    #(B1) = (n_1,1) which gets broadcasted to (n_1,m); hence (dB1) = (n_1,1)\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) #(dZ1) = (n_1,m) so sum along all the columns to get (n_1,1)\n",
    "    \n",
    "    grads ={\"dW2\": dW2, \"dB2\":dB2,\"dW1\": dW1, \"dB1\":dB1}\n",
    "    \n",
    "\n",
    "    return(grads,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_grad_desc(num_iterations, learning_rate,X,Y,n_1,n_2):\n",
    "    n_0, m = np.shape(X)\n",
    "    \n",
    "    W1 = init_parameters(n_1, n_0, random= True)\n",
    "    B1 = init_parameters(n_1,1, random = True)\n",
    "    \n",
    "    W2 = init_parameters(n_2, n_1, random= True)\n",
    "    B2 = init_parameters(n_2,1, random = True)\n",
    "    \n",
    "    loss_array = np.ones([num_iterations])*np.nan\n",
    "    for i in np.arange(num_iterations):\n",
    "        A2,A1 = fwd_prop(W1,B1,W2,B2,X,Y)\n",
    "    \n",
    "        grads,cost = back_prop(A1,W1,B1,A2,W2,B2,X,Y)\n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]\n",
    "        W2 = W2 - learning_rate*grads[\"dW2\"]\n",
    "        B2 = B2 - learning_rate*grads[\"dB2\"]\n",
    "\n",
    "        loss_array[i] = cost\n",
    "        \n",
    "        parameter = {\"W1\":W1,\"B1\":B1,\"W2\":W2,\"B2\":B2}\n",
    "    \n",
    "    return(parameter,loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = generate_cifar_train_test_set(x, y, classes, show_samples = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/animo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "/home/animo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "#Lets test on the CIFAR dataset\n",
    "params, loss_array = run_grad_desc(1000,1e-1,x_train, y_train, n_1 = 50, n_2 = 10)\n",
    "#Training data accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/animo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in less\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "y_train_predict,A1 = fwd_prop(params[\"W1\"],params[\"B1\"],params[\"W2\"],params[\"B2\"],x_train,y_train)\n",
    "y_train_predict_vector = np.argmax(y_train_predict, axis = 0)\n",
    "y_train_vector = np.argmax(y_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15975"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_train_predict_vector == y_train_vector)/len(y_train_vector)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14399999999999999"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test data accuracy\n",
    "y_predict,A1 = fwd_prop(params[\"W1\"],params[\"B1\"],params[\"W2\"],params[\"B2\"],x_test,y_test)\n",
    "y_test_predict_vector = np.argmax(y_predict, axis = 0)\n",
    "y_test_vector = np.argmax(y_test, axis = 0)\n",
    "accuracy = np.sum(y_test_predict_vector == y_test_vector)/len(y_test_vector)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efeb32638d0>]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAHVCAYAAADLiU4DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8nOV97/3vb0ajfV+t1buxjVcsvABhJ4VmMWn2EJJQ\nKOUkTUObPidpnz5JadpzmrYnp2lOCHVIgCckEEIIDW4gIcGEYONFNl6R912WLVnWblnrdf6YsZAd\nY0m2dN8j3Z/366WXZu65Bv3G18vWl2u7zTknAAAAjK6Q3wUAAAAEAaELAADAA4QuAAAADxC6AAAA\nPEDoAgAA8AChCwAAwAOELgAAAA8QugAAADxA6AIAAPBAgt8FXEh+fr6bNGmS32UAAAAMauPGjSed\ncwWDtYvL0DVp0iRVVVX5XQYAAMCgzOzQUNoxvQgAAOABQhcAAIAHCF0AAAAeIHQBAAB4gNAFAADg\ngUFDl5klm9l6M9tiZjvM7KELtLnLzLaa2TYzW2Nm8we8lm1mz5rZTjOrNrNlI/0hAAAA4t1Qjozo\nlHSzc67NzCKSXjezF51zawe0OSDpBudco5ndIWmFpCWx174p6SXn3IfMLFFS6kh+AAAAgLFg0NDl\nnHOS2mJPI7Evd16bNQOerpVUJklmliXpekmfibXrktR1uUUDAACMNUNa02VmYTPbLKlO0svOuXUX\naX6vpBdjjydLqpf0mJm9aWaPmlnaO/yM+82sysyq6uvrh/ERAAAA4t+QQpdzrtc5t0DREazFZjbn\nQu3M7CZFQ9eXYpcSJF0l6TvOuYWS2iV9+R1+xgrnXKVzrrKgYNCT9AEAAMaUYe1edM41SVol6fbz\nXzOzeZIelbTcOdcQu3xU0tEBI2PPKhrCAAAAAmUouxcLzCw79jhF0m2Sdp7XpkLSc5Luds7tPnvd\nOXdc0hEzuyJ26RZJb41Q7QAAAGPGUHYvFkt6wszCioa0Z5xzK83sAUlyzj0i6SuS8iQ9bGaS1OOc\nq4y9//OSfhjbubhf0j0j/BkAAADinkU3J8aXyspKV1VV5XcZAAAAgzKzjQMGm94RJ9IDAAB4gNAF\nAADggUCGrtYz3Wrv7PG7DAAAECCBDF0ffuQNPfjjzX6XAQAAAiSQoStkpjjcPwAAAMaxYIaukBSP\nuzYBAMD4FcjQZTL1EboAAICHAhm6QiYRuQAAgJcCGbrMTH2kLgAA4KGAhi7WdAEAAG8FMnSxexEA\nAHgtkKHLJBbSAwAATwUydIWM3YsAAMBbgQxd0TVdflcBAACChNAFAADggUCGrpCZHCd1AQAADwU2\ndHFOFwAA8FIgQ5cZuxcBAIC3Ahq6OKcLAAB4K5ChK8SJ9AAAwGOBDF3Rw1H9rgIAAARJIEMXuxcB\nAIDXAhm6zEx9fX5XAQAAgiSgoYvdiwAAwFuBDF0h87sCAAAQNAENXdzwGgAAeCuQoSs6veh3FQAA\nIEgCGrqMc7oAAICnAhm6QpxIDwAAPBbI0BU9HJXUBQAAvBPI0BUycTQqAADwVEBDF7sXAQCAtwIZ\numTiRHoAAOCpQIaukHE6KgAA8FZAQxcL6QEAgLcCGbpMrOkCAADeCmToCoXEOV0AAMBTgQxdZsZt\ngAAAgKeCGbokbgMEAAA8FcjQFTLjcFQAAOCpgIYudi8CAABvBTJ0mZn6WNQFAAA8FNDQxb0XAQCA\nt4IZumQcGQEAADwVyNAVMnYvAgAAbwUzdIU4pwsAAHgrkKHLxO5FAADgrWCGLs7pAgAAHgtk6GJN\nFwAA8FogQ5eZWNMFAAA8FcjQFTJjpAsAAHgqkKHLjN2LAADAW8EMXbHvjHYBAACvBDJ0hSwau8hc\nAADAK4OGLjNLNrP1ZrbFzHaY2UMXaHOXmW01s21mtsbM5g947WDs+mYzqxrpD3ApQrGhLs7qAgAA\nXkkYQptOSTc759rMLCLpdTN70Tm3dkCbA5JucM41mtkdklZIWjLg9ZuccydHruzLY/2hy986AABA\ncAwaulx04VNb7Gkk9uXOa7NmwNO1kspGqsDRYGenFzkiFQAAeGRIa7rMLGxmmyXVSXrZObfuIs3v\nlfTigOdO0q/NbKOZ3X+Rn3G/mVWZWVV9ff1QyrpkrOkCAABeG1Locs71OucWKDqCtdjM5lyonZnd\npGjo+tKAy9fF3nuHpM+Z2fXv8DNWOOcqnXOVBQUFw/oQw2Ws6QIAAB4b1u5F51yTpFWSbj//NTOb\nJ+lRScudcw0D3lMT+14n6WeSFl9OwSPh7EJ6MhcAAPDKUHYvFphZduxxiqTbJO08r02FpOck3e2c\n2z3gepqZZZx9LOndkraPXPmX5uz0IiNdAADAK0PZvVgs6QkzCysa0p5xzq00swckyTn3iKSvSMqT\n9HBskXqPc65SUpGkn8WuJUj6kXPupZH/GJeG3YsAAMArQ9m9uFXSwgtcf2TA4/sk3XeBNvslzT//\nut/OjnSxeREAAHgloCfSR78zvQgAALwSyNBlrOkCAAAeC2ToCjG7CAAAPBbI0MVIFwAA8FpAQ1f0\nO5kLAAB4JZChi9sAAQAArwU0dEW/M70IAAC8EsjQZWJNFwAA8FYwQxdrugAAgMcCGrpY0wUAALwV\nyNDFmi4AAOC1gIau2EiXz3UAAIDgCGToMka6AACAxwIauljTBQAAvBXI0NV/70VSFwAA8EggQ9fb\n53T5XAgAAAiMQIau/pEultIDAACPBDJ0nV3T1dfncyEAACAwAhq6ot/ZvQgAALwSyNB19pwuQhcA\nAPBKIENXQmxRVy8r6QEAgEcCGbpCIUa6AACAtwIZusJ2dqTL50IAAEBgBDJ0hWKfmulFAADglUCG\nrjAL6QEAgMcCGboSwiykBwAA3gpk6AoZoQsAAHgrkKErzJERAADAY4EMXf0jXazpAgAAHglk6Do7\n0tXHSBcAAPBIIENX/4n0jHQBAACPBDJ0hVjTBQAAPBbI0BVm9yIAAPBYMEMXI10AAMBjgQxd3PAa\nAAB4LZChixteAwAArwUzdPVPL5K6AACANwIeupheBAAA3ghm6Oo/kd7nQgAAQGAEMnSFYp+aE+kB\nAIBXAhm6wpxIDwAAPBbs0MVIFwAA8EgwQxcn0gMAAI8FM3Qx0gUAADwWyNBlZjLjRHoAAOCdQIYu\nKTrFyEgXAADwSnBDV8jYvQgAADwT7NDF6agAAMAjwQ1dxkgXAADwTmBDVyhknEgPAAA8E9jQxZou\nAADgpcCGrpCZevv8rgIAAARFYENXQsjU20fqAgAA3ghs6AqHGOkCAADeGTR0mVmyma03sy1mtsPM\nHrpAm7vMbKuZbTOzNWY2/7zXw2b2ppmtHMniL0coxIn0AADAOwlDaNMp6WbnXJuZRSS9bmYvOufW\nDmhzQNINzrlGM7tD0gpJSwa8/gVJ1ZIyR6rwy8WJ9AAAwEuDjnS5qLbY00jsy53XZo1zrjH2dK2k\nsrOvmVmZpPdIenREKh4hIXYvAgAADw1pTVdsenCzpDpJLzvn1l2k+b2SXhzw/N8k/XdJF11BZWb3\nm1mVmVXV19cPpazLksA5XQAAwENDCl3OuV7n3AJFR7AWm9mcC7Uzs5sUDV1fij1/r6Q659zGIfyM\nFc65SudcZUFBwZA/wKUKmamH0AUAADwyrN2LzrkmSask3X7+a2Y2T9EpxOXOuYbY5Wslvd/MDkp6\nWtLNZvbkZVU8QsKMdAEAAA8NZfdigZllxx6nSLpN0s7z2lRIek7S3c653WevO+f+2jlX5pybJOlj\nkl5xzn1yBOu/ZAnhECNdAADAM0PZvVgs6QkzCysa0p5xzq00swckyTn3iKSvSMqT9LCZSVKPc65y\nlGoeEYlhUzcHdQEAAI8MGrqcc1slLbzA9UcGPL5P0n2D/HdelfTqsCscJZFwiNAFAAA8E9gT6SPh\nkLp6mV4EAADeCHTo6u5hpAsAAHgjsKErMYE1XQAAwDuBDV2s6QIAAF4KeOhiTRcAAPBGoENXFyNd\nAADAI4ENXZzTBQAAvBTY0MXuRQAA4KXghq4E1nQBAADvBDd0xdZ0OUfwAgAAoy+4oStkksRNrwEA\ngCeCG7oSoh+dxfQAAMALwQ1d4Vjo6mGkCwAAjL7Ahq7EcHR6kbO6AACAFwIbuvpHughdAADAA4EP\nXT0cGwEAADwQ3NAVW0jP9CIAAPBCYEPX2TVdTC8CAAAvBDZ0nZ1e7OJWQAAAwAOBDV3JkbAk6Ux3\nr8+VAACAIAhs6EpJjIau04QuAADggcCGrtRY6OroInQBAIDRF9jQlRIhdAEAAO8EN3QxvQgAADwU\n2NCVmpggSero6vG5EgAAEASBDV1vTy9yZAQAABh9gQ1d4ZApMSGk092MdAEAgNEX2NAlRXcwspAe\nAAB4IdihK0LoAgAA3gh06EpODLN7EQAAeCLQoYvpRQAA4JVAh66MpIhaz3T7XQYAAAiAQIeu7NSI\nmk4TugAAwOgjdHUQugAAwOgLdOjKSklU8+luOef8LgUAAIxzgQ5d2akRdfX2qYMdjAAAYJQFO3Sl\nRCSJdV0AAGDUBTt0pRK6AACANwIdunLTkiRJ9W2dPlcCAADGu0CHruKsZEnSieYzPlcCAADGu0CH\nrsLM6EhXLaELAACMskCHrqSEsPLTk1Tb3OF3KQAAYJwLdOiSolOMxxjpAgAAoyzwoWtiXqoOnGzz\nuwwAADDOBT50TStM19HGDp3hgFQAADCKAh+6phakyzlpf32736UAAIBxLPCha3ZJpiRpW02Tz5UA\nAIDxLPCha0p+mrJTI9p4qNHvUgAAwDgW+NBlZlpUkUPoAgAAoyrwoUuSrpqYo3317Wo63eV3KQAA\nYJwidEmqnJgjSVq7/5TPlQAAgPGK0KXoSFdmcoJ+U33C71IAAMA4ReiSFAmHdNPMQr2ys069fc7v\ncgAAwDhE6Iq5dVaRGtq79OZhFtQDAICRN2joMrNkM1tvZlvMbIeZPXSBNneZ2VYz22Zma8xs/lDf\nGy9uuKJAkbDpZaYYAQDAKBjKSFenpJudc/MlLZB0u5ktPa/NAUk3OOfmSvqapBXDeG9cyEyOaOmU\nPL20/bicY4oRAACMrEFDl4s6e0foSOzLnddmjXPu7LzcWkllQ31vPHnf/BIdajitzUc4nR4AAIys\nIa3pMrOwmW2WVCfpZefcuos0v1fSi8N9r5ndb2ZVZlZVX18/9E8wgm6fM0GJCSH95+Zjvvx8AAAw\nfg0pdDnnep1zCxQdwVpsZnMu1M7MblI0dH1puO91zq1wzlU65yoLCgqG+zlGRGZyRLfNKtILW46p\nu7fPlxoAAMD4NKzdi865JkmrJN1+/mtmNk/So5KWO+cahvPeeLJ8QYka2rv0+t6TfpcCAADGkaHs\nXiwws+zY4xRJt0naeV6bCknPSbrbObd7OO+NNzdeUaislIief7PG71IAAMA4kjCENsWSnjCzsKIh\n7Rnn3Eoze0CSnHOPSPqKpDxJD5uZJPU45yrf6b2j8DlGTGJCSO+ZV6yfbapRe2eP0pKG8kcEAABw\ncYMmCufcVkkLL3D9kQGP75N031DfG+/uXFCqH607rJe2H9cHF5X5XQ4AABgHOJH+Aq6elKNJean6\ncdURv0sBAADjBKHrAsxMH64s1/oDp3TgZLvf5QAAgHGA0PUOPrSoTCGTnmG0CwAAjABC1zsoykzW\nzTML9dONR9XDmV0AAOAyEbou4iOV5apr7dSru/w5IR8AAIwfhK6LuGlmofLTk/T0BqYYAQDA5SF0\nXUQkHNIHF5Vq1a461bWc8bscAAAwhhG6BvHRynL19jn9dBMn1AMAgEtH6BrElIJ0LZ6Uq2eqjqiv\nz/ldDgAAGKMIXUPwiSUVOnCyXav3cRNsAABwaQhdQ3DH3AnKS0vUD9445HcpAABgjCJ0DUFSQlgf\nvbpcv64+oZqmDr/LAQAAYxCha4g+saRCkvTUusM+VwIAAMYiQtcQleWk6uaZRXp6w2F19vT6XQ4A\nABhjCF3DcPeyiTrZ1qWXth/3uxQAADDGELqG4V3T8jUpL1VPrmVBPQAAGB5C1zCEQqZPLp2oDQcb\n9daxFr/LAQAAYwiha5g+vKhcKZGwnlhz0O9SAADAGELoGqas1Ig+uKhUP9tco/rWTr/LAQAAYwSh\n6xL88bWT1dXTx9ouAAAwZISuSzClIF23zirUk2sP6Uw3x0cAAIDBEbou0b3XTVFDe5eef7PG71IA\nAMAYQOi6REun5OrKkkx97/UDcs75XQ4AAIhzhK5LZGa6712TtaeuTa/tOel3OQAAIM4Rui7De+aW\nqCgzSY/+br/fpQAAgDhH6LoMiQkhffqaSfrdnpPaXtPsdzkAACCOEbou0yeXTlRGUoK+89t9fpcC\nAADiGKHrMmUmR/SpaybqF9tqtb++ze9yAABAnCJ0jYB7rp2sxHBIjzDaBQAA3gGhawTkpyfp44sr\n9NymGtU0dfhdDgAAiEOErhHyJ9dPkSR99zV2MgIAgN9H6Bohpdkp+sDCUj294bAa2rgRNgAAOBeh\nawQ9cONUdfb06XuvH/C7FAAAEGcIXSNoakG63juvRI+vOchoFwAAOAeha4R94ZbpOtPdqxWs7QIA\nAAMQukbYtMJ0LV9QqifeOKj6Vka7AABAFKFrFPz5LdPV3es4twsAAPQjdI2Cyflp+sDCUj259pBO\ntJzxuxwAABAHCF2j5M9vnq7ePqeHV+31uxQAABAHCF2jpCIvVR+uLNNT64/oaONpv8sBAAA+I3SN\nos/fPF0y6Ru/2u13KQAAwGeErlFUkp2ie66ZpJ9trtFbx1r8LgcAAPiI0DXKPnvjNGUmR/T1l3b6\nXQoAAPARoWuUZaVG9Lmbpuq3u+u1Zu9Jv8sBAAA+IXR54FPLJqk0O0X/88Wd6utzfpcDAAB8QOjy\nQHIkrL+8bYa21TRr5bZav8sBAAA+IHR55M6FpZpVnKmvv7hTZ7p7/S4HAAB4jNDlkXDI9JX3zlZN\nUwc3wwYAIIAIXR5aNjVPfzh3gh5+da+ONXX4XQ4AAPAQoctjf33HLDkn/dOLHCEBAECQELo8Vp6b\nqj+9fop+vuWYNhw85Xc5AADAI4QuHzxw41QVZyXr736+Q70cIQEAQCAQunyQmpigL98xUzuOtehH\n6w/7XQ4AAPAAocsn759fomun5emfX9ypupYzfpcDAABG2aChy8ySzWy9mW0xsx1m9tAF2txlZlvN\nbJuZrTGz+bHr5Wa2yszeir33C6PxIcYiM9M/3DlXnb19+vuVb/ldDgAAGGVDGenqlHSzc26+pAWS\nbjezpee1OSDpBufcXElfk7Qidr1H0hedc7MlLZX0OTObPTKlj32T89P0uRunaeXWWv12d73f5QAA\ngFE0aOhyUW2xp5HYlzuvzRrnXGPs6VpJZbHrtc65TbHHrZKqJZWOUO3jwgM3TtGUgjT97fPb1NHF\nSfUAAIxXQ1rTZWZhM9ssqU7Sy865dRdpfq+kFy/w35gkaaGkC77XzO43syozq6qvD86oT1JCWP94\n51wdOdWhb72yx+9yAADAKBlS6HLO9TrnFig6grXYzOZcqJ2Z3aRo6PrSedfTJf1U0oPOuZZ3+Bkr\nnHOVzrnKgoKC4XyGMW/Z1Dx98KoyrXhtv3Yev+AfDwAAGOOGtXvROdckaZWk289/zczmSXpU0nLn\nXMOA6xFFA9cPnXPPXV6549f/+55ZykqJ6P/5yVZ19/b5XQ4AABhhQ9m9WGBm2bHHKZJuk7TzvDYV\nkp6TdLdzbveA6ybpe5KqnXPfGMnCx5vctET9w51ztK2mWY+8us/vcgAAwAgbykhXsaRVZrZV0gZF\n13StNLMHzOyBWJuvSMqT9LCZbTazqtj1ayXdLenm2PXNZvaHI/0hxos75hbrffNL9O+v7FF1LdOM\nAACMJ+Zc/N2GprKy0lVVVQ3ecBw61d6ld//v36ooM1nPf+5aRcKcXwsAQDwzs43OucrB2vEbPc5E\npxnnasexFj28imlGAADGC0JXHLp9zgQtX1Cib72yR9trmv0uBwAAjABCV5z6u/ddqbz0RH3h6Td1\nuqvH73IAAMBlInTFqZy0RP3vjyzQ/pPt+hr3ZgQAYMwjdMWxa6bl64Ebpuqp9Uf04rZav8sBAACX\ngdAV5/7ythmaX5alLz+3TceaOvwuBwAAXCJCV5yLhEP65scWqqe3Tw/+eLN6++LviA8AADA4QtcY\nMCk/TX+/fI7WHzilb/6Gm2IDADAWEbrGiD+6qlQfWlSmb72yR6t21fldDgAAGCZC1xhhZvra8jma\nOSFTf/HjzTpy6rTfJQEAgGEgdI0hKYlhPfLJq9Tb5/TZH27Sme5ev0sCAABDROgaYybmpekbH1mg\nbTXNeugFzu8CAGCsIHSNQbfNLtJnb5yqp9Yf1o83HPa7HAAAMASErjHqL2+boXdNz9ffPr9d6w+c\n8rscAAAwCELXGJUQDun/fPwqleek6oEnN7KwHgCAOEfoGsOyUiN69NOV6unt071PbFDrmW6/SwIA\nAO+A0DXGTSlI18N3LdK++nZ94WlOrAcAIF4RusaB66bn6+/eN1uv7KzTP/zXW3KO4AUAQLxJ8LsA\njIy7l03S/pPtemz1QZVkpehPrp/id0kAAGAAQtc48v+9Z7bqWjr1j7+oVmFmkpYvKPW7JAAAEEPo\nGkdCIdP/+sh8nWzr1F/9ZIvy05N07bR8v8sCAABiTde4kxwJa8WnKjUlP11/+oON2l7T7HdJAABA\nhK5xKSslosf/+GplJifo099fr711rX6XBABA4BG6xqnirBQ9ed8SmZnuenSdDjW0+10SAACBRuga\nx6YUpOuH9y1RV0+fPvHddapp6vC7JAAAAovQNc5dMSFDP7h3iVrOdOuu765VXcsZv0sCACCQCF0B\nMKc0S4/fs1h1rZ362HfX6ngzwQsAAK8RugJi0cQcPX7PYp1oPqOPrniDqUYAADxG6AqQxZNz9YP7\nluhUe5c+8sgbOtxw2u+SAAAIDEJXwFxVkaMf3bdU7V09+sh/vKH99W1+lwQAQCAQugJoblmWnvqT\nperu7dNH/mOt3jrW4ndJAACMe4SugJpVnKkf/+lSRcKmj/7HG1qz76TfJQEAMK4RugJsWmGGfvrf\nrtGErGR95vsbtHLrMb9LAgBg3CJ0BVxJdoqefeAazS/P0uefelOPrT7gd0kAAIxLhC4oKzWiH9y7\nRO+eXaSHXnhL/+MX1ertc36XBQDAuELogiQpORLWw3ct0t1LJ2rFa/v1pz+oUltnj99lAQAwbhC6\n0C8cMv398iv10Puv1Kpd9frQd9boyCnO8gIAYCQQunAOM9Onr5mkx++5WjVNHbrz26tVdfCU32UB\nADDmEbpwQe+aXqDnP3etMlMi+vh31+pH6w7LOdZ5AQBwqQhdeEdTC9L1/Gev1bKp+fqbn23TX/1k\nqzq6ev0uCwCAMYnQhYvKSo3osc9crQdvna7n3jyqDzy8WgdOtvtdFgAAYw6hC4MKh0wP3jpDj33m\nah1vOaP3f+t1vbS91u+yAAAYUwhdGLIbryjUys9fp8kFaXrgyU36m59tY7oRAIAhInRhWMpyUvWT\nB5bp/uun6EfrDuu93/qdttc0+10WAABxj9CFYUtKCOtv/nCWnrx3iVrP9OgDD6/Wd1/brz5OsQcA\n4B0RunDJrpuer5cevF43XlGof/xFtT71/fWqaerwuywAAOISoQuXJTctUSvuXqR//MAcbTrcqHd/\n47d6cu0hRr0AADgPoQuXzcx015KJ+uWD12tBRbb+9vntuuvRdTrcwC2EAAA4i9CFEVOem6on712i\n//GBudpW06w/+LfX9NjqA+pl1AsAAEIXRpaZ6RNLKvSrv7heiyfn6qEX3tKd316tLUea/C4NAABf\nEbowKkqyU/T4PVfr3z++UMdbzujOh1frb5/fpubT3X6XBgCALwhdGDVmpvfPL9FvvniDPnPNJP1o\n3WHd8o1X9dONR1loDwAIHEIXRl1mckRffd+VeuHz16k8N1Vf/MkWfeA7a1R18JTfpQEA4BlCFzxz\nZUmWfvrANfrXD8/X8eYOfeiRN/S5H25ilyMAIBAGDV1mlmxm681si5ntMLOHLtDmLjPbambbzGyN\nmc0f8Nr3zazOzLaPdPEYe0Ih04cWlWnVX92oB2+drld21unWb/xW//MX1WruYL0XAGD8GspIV6ek\nm51z8yUtkHS7mS09r80BSTc45+ZK+pqkFQNee1zS7SNQK8aR1MQEPXjrDK36qxv1/gUlWvG7/brh\nX1bpO6/u0+muHr/LAwBgxA0aulxUW+xpJPblzmuzxjnXGHu6VlLZgNdek8TiHVzQhKxk/euH5+uF\nP7tOC8qz9fWXdur6f35Vj60+oM6eXr/LAwBgxAxpTZeZhc1ss6Q6SS8759ZdpPm9kl4cbiFmdr+Z\nVZlZVX19/XDfjjFuTmmWHr9nsZ59YJmmFabpoRfe0k3/8qqeXn9Y3b19fpcHAMBlM+eGvnXfzLIl\n/UzS551zv7dGy8xukvSwpOuccw0Drk+StNI5N2coP6eystJVVVUNuS6ML845rd7boH/51S5tOdKk\n0uwU3X/9FH306nIlR8J+lwcAwDnMbKNzrnKwdsPaveica5K0ShdYo2Vm8yQ9Kmn5wMAFDJeZ6brp\n+Xr+s9fosc9crQlZyfrqz3fouq+/oodf3auWMyy4BwCMPUPZvVgQG+GSmaVIuk3SzvPaVEh6TtLd\nzrndo1EogsfMdNPMQj37wDL9+P6lml2SpX9+aZeu/adX9K+/3KX61k6/SwQAYMgGnV6MjWA9ISms\naEh7xjn392b2gCQ55x4xs0clfVDSodjbes4Os5nZU5JulJQv6YSkrzrnvnexn8n0It7JtqPN+vaq\nvXppx3ElhkN6/4IS3XPtJF1ZkuV3aQCAgBrq9OKw1nR5hdCFweyrb9Pjqw/q2Y1H1dHdq6VTcvXH\n107WLbOKFA6Z3+UBAAKE0IVAaD7drR9XHdYTaw6ppqlDFbmp+tSyifrQojJlpyb6XR4AIAAIXQiU\nnt4+/eqtE/r+6wdUdahRSQkhvWdusT6+pEKVE3NkxugXAGB0ELoQWG8da9FT6w/r+Tdr1NrZo+mF\n6fr44gqW1/mWAAAXm0lEQVT90VWljH4BAEYcoQuBd7qrRyu31OqH6w9ry5EmJSWEdPucCfqjq8p0\n7dQ8JYS53zsA4PIRuoABdhxr1tPrj+jnW46puaNbhRlJunNhqf7oqlLNnJDpd3kAgDGM0AVcQGdP\nr1btrNNPN9Vo1c469fQ5zSrO1AevKtX755eoMDPZ7xIBAGMMoQsYxKn2Lq3cekw/3VSjLUeaZCYt\nnpSr984r1h/MmaDCDAIYAGBwhC5gGPbWtWnl1mNaubVWe+vaFDJp8eRcvWdeiW6/coIKMpL8LhEA\nEKcIXcAl2n2iVSu31uq/th7Tvvp2hUxaMjlPf3BlkW6dXaSynFS/SwQAxBFCF3CZnHPadaJVv9ha\nq5XbarW/vl2SNLs4U7fOLtK7ZxfpypJMzgADgIAjdAEjbF99m3791gn9uvqENh5qVJ+TirOSdeus\n6AjY0im5SkoI+10mAMBjhC5gFDW0deqVnXX6dfUJvbb7pDq6e5USCWvZ1DzdMKNAN8wo0KT8NL/L\nBAB4gNAFeORMd6/W7Dup3+6q12931+tgw2lJ0sS81P4AtnRKntKSEnyuFAAwGghdgE8ONbTrtd3R\nALZmX4NOd/UqEjZVTszVtdPytGxqvuaVZSnCifgAMC4QuoA40NnTq40HG/Xb3fV6bc9JVde2SJLS\nEsO6enKulk3J0zVT8zW7JFPhEAvyAWAsInQBcehUe5fW7W/QG/sbtGZfg/bWtUmSMpMTtGRKnq6Z\nmqelU/I0oyiDEAYAY8RQQxeLTAAP5aYl6o65xbpjbrEkqa7ljN7Y36A39kVD2MtvnZAkZSQnaNHE\nHF09KVeVE3M0vzxbyRF2RgLAWEboAnxUmJms5QtKtXxBqSTpaONpbTh4ShsONmrDgVN6ddcuSVIk\nbJpbmhUNYZNytWhijnLTEv0sHQAwTEwvAnGssb1LGw81asOhU6o62KitR5vU3Rv9OzutMF0Ly7O1\noCJbC8tzNKMoXQkszgcAz7GmCxiHznT3auvRZm04eEpVB09p85EmNZ7uliSlRMKaW5YVDWKxMFac\nleJzxQAw/rGmCxiHkiNhLZ6cq8WTcyVFb1V0+NRpvXm4SZuPNOnNI016bPVBdfX2SZKKMpOiAaw8\nRwvKszWnNFMZyRE/PwIABBahCxjDzEwT89I0MS9Ndy6Mrgvr7OnVW8datPlIU//XL3ec6H/P5Pw0\nzSnN0pySTM0tzdKVJVnKSiWIAcBoI3QB40xSQlgLK3K0sCKn/9qp9i5tOdqkHTXN2lbTrE2HGvXC\nlmP9r1fkpmpOaabmlGZpbmmW5pRkKYeF+gAwoghdQADkpiXqpisKddMVhf3XTrV3aXtNs7Yfa45+\nr2nRL7Yd73+9NDslGsRKsjSzOFOzijNUmp0iM84PA4BLQegCAio3LVHXzyjQ9TMK+q81n+7WjmPR\n0bDtx1q0vab5nKnJzOQEzSzO1OxYCJtVnKkZRRmcIQYAQ0DoAtAvKzWia6bl65pp+f3X2jp7tOt4\ni6prW1Vd26Lq2hY9U3VEp7t6JUkhi64Tm1WcqVn9gSxTRZlJjIoBwACELgAXlZ6UoEUTc7VoYm7/\ntb6+6K7J6toWVR+PhrHNR5q0cmttf5uc1IhmTsjUzOIMXVGUoelFGZpRlM7uSQCBRegCMGyhkGlS\nfpom5af139JIklrOdGvngBGx6toW/XjD26NiUnSt2IyidM2YkKEZhRm6YkKGphWmM0UJYNwjdAEY\nMZnJkXPOEZOio2I1TR3adbxVu060aveJVu0+0abVexv6zxMLmTQxLy0axooyNKMoGsYm56cpwin7\nAMYJQheAURUKmcpzU1Wem6pbZxf1X+/p7dPBhtOxEBb92nW8Vb+urlNvX/ROGZGwaUp+uqYXpeuK\nogzNmJCh6YXpqshN5ZZHAMYcQhcAXySEQ5pWmK5phen6wwFTlJ09vdpf394fwnafaNXWo83nrBdL\nDIc0OT9N04rSNa0gGsqmFaZrcn6akhKYpgQQnwhdAOJKUkK4fyfkQO2dPdpb16a9dW3aU9emvXWt\n2lHTrBe31So2MNY/TTmtMF3TY4FuemGGphamKTWRf+4A+It/hQCMCWlJCZpfnq355dnnXD/T3asD\nJ9ujQexEq/bWt2nPiTa9uqtO3b2uv11pdsq5YawoXdMKMrgFEgDPELoAjGnJkQuPjHX39ulQw2nt\nrWsdMDrWprX7G9TZ09ffriAjacCoWLqmxkbH8tMTOWcMwIgidAEYlyID1owN1NvnVNPYob31rdpz\n4u3pyuc21aits6e/XVZKRNNjI2JTC9I1vSh6tEVJVjJhDMAlIXQBCJRwyFSRl6qKvFTdPPPt3ZTO\nOZ1o6dSe80bGXtp+XI2nu/vbpSWGNXXAerGzI2TluakKhwhjAN4ZoQsAJJmZJmQla0JWst41veCc\n1xraOs8JYnvr2rRmb4Oe21TT3yYxIaQp+Wn9YezsjspJeWlKTOB4CwCELgAYVF56kvLSk7RkSt45\n11vOdPeHsLNfW4426b+21crF1vCHQ6aJeann7KacVhidskxJ5HgLIEgIXQBwiTKTI7qqIkdXVeSc\nc72jq1f76tu0L7aT8uyU5cCDX82kspyU2DljGZpWkB49d6wwXZncnxIYlwhdADDCUhLDmlOapTml\nWedc7+rp06GG9v5pyrPfV+9rUNeAHZVFmUn9I2LTBuyszEtP8vqjABhBhC4A8EhiQkjTizI0vSjj\nnOu9fU5HG0/HRsXOTlW26idVR9Q+4GbhOamR2GGv6f07K6cVpmtCJjsqgbGA0AUAPouu+0rTxLy0\nc+5P6ZxTbfOZ8xbxt+rF7bV6asCOyvSkhLeD2IC1Y6U5KeyoBOIIoQsA4pSZqSQ7RSXZKbp+xts7\nKp1zamjvip4zVv/2Sfyv7a7XsxuP9rdLSghpakH6753EPzEvTRFuGA54jtAFAGOMmSk/PUn56Ula\nNvXcHZXNHd39I2JnR8g2HW7Uz7cc628TCZumFqRrRlGGrpiQEf1elKGynBSFGBkDRg2hCwDGkayU\niBZNzNGiiefuqDzd1aP99e3aU9eq3SfatPt46++FsZRIWDOKzgtjEzJUmJHEmjFgBBC6ACAAUhMT\nLrijsq2zR7tPtGr38VbtOtGq3SdatWpXvX4yYJoyKyWiK4oyNGNCuq6YkBl9XJSu7NRErz8GMKYR\nugAgwNKTEi541lhDW2d0ROxELIwdb9V/bj6m1jOH+9sUZSb1T03OmBD9Pr0oXamJ/GoBLoS/GQCA\n35OXnqRl560Zc87peMsZ7Tx+7sjYD9YeUueAc8YqclM1oyhDs4ozNKs4U7OKMzUxN5X1Ygg8QhcA\nYEjMTMVZKSrOStFNVxT2X+/tczp86rR2HW/tHxnbWduiV3aeUOwAfqVEwrpiwtkQFv0+c0KGMjh9\nHwFi7uwNwuJIZWWlq6qq8rsMAMBlONPdq90nWrWztlVv1baoOvbVcqanv01ZTkr/aNisWCirYFQM\nY4yZbXTOVQ7WjpEuAMCoSI6ENa8sW/PKsvuvnT3wtbq2RTuPR8PYztoW/ab67VGx1MSBo2LRMDaz\nOFPpSfzKwtjGSBcAwHcdXbFRseMtqq5tveCoWEVuqmZOyNCVJVmaU5qpK0uyVJTJcRbwHyNdAIAx\nIyUxrPnl2ZpffuFRseraFlUfb1X1sRa9XH1CZ8cL8tISdWVplq4sydSVJZmaU5LF9CTiFqELABCX\nBt4G6ZZZb9+Tsr2zR9W1LdpxrEXba5q141iLvvvafvXE5ifTkxI0uzhTV8ZGw+aUZmpqQTq3PoLv\nBg1dZpYs6TVJSbH2zzrnvnpem7skfUmSSWqV9N+cc1tir90u6ZuSwpIedc7904h+AgBAoKQlJahy\nUq4qJ+X2X+vs6dWeE23acaxZ22tatONYs55ef0Qd3QclSYkJof6pybOjYrOKM5UcCfv0KRBEg67p\nsuhkeZpzrs3MIpJel/QF59zaAW2ukVTtnGs0szsk/Z1zbomZhSXtlnSbpKOSNkj6uHPurYv9TNZ0\nAQAuV2+f04GTbeeMiO041qLmjm5JUjhkmlqQpjklWZpblqV5ZVmaXZyllESCGIZnxNZ0uWgqa4s9\njcS+3Hlt1gx4ulZSWezxYkl7nXP7Y0U9LWm5pIuGLgAALlc4ZJpWmKFphRlavqBUUnSd2NHGjlgA\niwax3+09qeferOl/z/TCdM0ry9LcsmzNK83SzOIMJSUQxHD5hrSmKzZitVHSNEnfds6tu0jzeyW9\nGHtcKunIgNeOSlryDj/jfkn3S1JFRcVQygIAYFjMTOW5qSrPTdXtcyb0Xz/RckZbjzZr69EmbT3a\nrF9X1+mZquj9JyNh08wJmdHRsNLoqNiMogzWiGHYhhS6nHO9khaYWbakn5nZHOfc9vPbmdlNioau\n64ZbiHNuhaQVUnR6cbjvBwDgUhVlJuu22cm6bXZ0wb5zTjVNHdp2tFlbjjZrW02TXthyTD9aF733\nZFJCSLNLMmMhLFvzyrI0tSBdYXZN4iKGtXvROddkZqsk3S7pnNBlZvMkPSrpDudcQ+xyjaTyAc3K\nYtcAAIhbZqaynFSV5aTqjrnFkqS+2O2Othxt0rajzdpa06yfbDyqJ944JCl6qOvZ9WHzy7O1sDxb\nZTkpnCOGfkNZSF8gqTsWuFIk/UrS151zKwe0qZD0iqRPDVzfZWYJii6kv0XRsLVB0iecczsu9jNZ\nSA8AGAvOLtbfcqRZ22qi05M7jrX03wA8Ly1RC8qztaA8WwsrcjSvPEuZ3G9y3BnJw1GLJT0RW9cV\nkvSMc26lmT0gSc65RyR9RVKepIdjib7HOVfpnOsxsz+T9EtFj4z4/mCBCwCAsWLgYv0PLoruIevu\n7dOu461680iTNh9u0uYjjfrNzjpJkpk0tSC9P4gtKM/WzAkZSmB9WCBwGyAAAEZZc0e3th6NhrA3\njzRp85EmnWrvkiSlRMKaW5qlBRVvB7HirGSmJccQbgMEAECcyEqJ6F3TC/Su6QWSogv1j5zq0JtH\nGrU5FsIeX31QXb3RacnCjCQtrMjWgvIcLSjP1vzyLKUm8it7rKMHAQDwmJmpIi9VFXmp/WeIdfb0\nqrq2VZsPvx3EfrnjhKToNObs4kwtmpjT/1WSneLnR8AlYHoRAIA4daq9S5uPNGrToSZtPBQNYx3d\nvZKk4qxkXTUxR4sqclQ5KUezijM5O8wnTC8CADDG5aYl6uaZRbp5ZvT8sO7ePu2sbdXGQ6e08XCT\nNh1q1H9trZUkJUdCml+WrUUToyFsYXmOctIS/Swf52GkCwCAMay2uUMbDzVq46FGbTrUqB3HWtTT\nF/3dPrUgLRrCJubqqok5mlqQxgL9UTDUkS5CFwAA40hHV6+2HG3qD2EbDzeq6XT0Jt/ZqRFVTszR\n1ZNytXhyruaUZjElOQKYXgQAIIBSEsNaOiVPS6fkSYrulNxX365NhxpVdeiUqg426tfV0XPDUiJh\nLazI1uLJuVo8KVcLK3KUksjNvUcLI10AAARMXesZVR1s1PoDp7T+wClVH2+Rc1JCyDS3LEuLYyNh\nlRNzlZXKCfqDYXoRAAAMSXNHtzYdatT6g9EQtvVok7p7ncykK4oytHhybv+UZFFmst/lxh1CFwAA\nuCRnunu1+UiT1h84pQ0HT2njoUad7ooeVTExL7U/gC2dnKfyXG7qzZouAABwSZIj564L6+nt045j\nLdpw8JTWHTil31Sf0LMbj0qSSrNTYm1ztWxqnspyUv0sPa4x0gUAAIalr89pb32b1u5v0Bv7GrR2\nf4MaYzsky3NTtHRynpZNjYa2IJycz/QiAADwRF+f0+66Vq3d16A39jdo3YFT/cdUTMxL1bLYqNmy\nqXnjck0YoQsAAPiir89p5/FWvREbCVt/oEEtZ3okSVPy07RkytmRsFwVZoz9EEboAgAAcaG3z6m6\ntqV/OnL9gVNq7YyGsKkFabpuWr6unZavpVPzlJk89o6oIHQBAIC41NvntONYs9bub9DqvdEQ1tHd\nq5BJ88uz+0PYwopsJSXE/2GthC4AADAmdPX06c3DjVq996Re33tSW442q7fPKSUS1uLJuf0hbOaE\nDIVC8Xc8BaELAACMSS1nurVu/ymt3ntSv9tTr3317ZKkvLREXTMtX9dNy9O10/Lj5ngKzukCAABj\nUmZyRLfNLtJts4skSbXNHVq9t6F/JOyFLcckSZPyUnXttHy9a3q+lk3NV1ZKfK8HY6QLAACMGc45\n7alr0+t7Tmr13pNau79B7V29CodMC8qz9a7p+bp+RoHmlWYpIRzypCamFwEAwLjX3dunNw836Xd7\n6vXanpPaerRJzkmZyQm6bnq+vvGRBUqOjO5ifKYXAQDAuBcJh7R4cvRekF989xVqbO/S6n0n9dru\neh1sOD3qgWs4CF0AAGDcyElL1Hvnlei980r8LuX3eDPZCQAAEHCELgAAAA8QugAAADxA6AIAAPAA\noQsAAMADhC4AAAAPELoAAAA8QOgCAADwAKELAADAA4QuAAAADxC6AAAAPEDoAgAA8AChCwAAwAOE\nLgAAAA8QugAAADxA6AIAAPAAoQsAAMADhC4AAAAPmHPO7xp+j5nVSzo0yj8mX9LJUf4ZGB76JD7R\nL/GJfok/9El88qJfJjrnCgZrFJehywtmVuWcq/S7DryNPolP9Et8ol/iD30Sn+KpX5heBAAA8ACh\nCwAAwANBDl0r/C4Av4c+iU/0S3yiX+IPfRKf4qZfArumCwAAwEtBHukCAADwDKELAADAA4ELXWZ2\nu5ntMrO9ZvZlv+sJEjMrN7NVZvaWme0wsy/Eruea2ctmtif2PWfAe/461le7zOwP/Kt+fDOzsJm9\naWYrY8/pE5+ZWbaZPWtmO82s2syW0S/+MrO/iP3btd3MnjKzZPrEe2b2fTOrM7PtA64Nux/MbJGZ\nbYu99u9mZqNde6BCl5mFJX1b0h2SZkv6uJnN9reqQOmR9EXn3GxJSyV9Lvbn/2VJv3HOTZf0m9hz\nxV77mKQrJd0u6eFYH2LkfUFS9YDn9In/vinpJefcTEnzFe0f+sUnZlYq6c8lVTrn5kgKK/pnTp94\n73FF/0wHupR++I6kP5E0PfZ1/n9zxAUqdElaLGmvc26/c65L0tOSlvtcU2A452qdc5tij1sV/SVS\nqmgfPBFr9oSkO2OPl0t62jnX6Zw7IGmvon2IEWRmZZLeI+nRAZfpEx+ZWZak6yV9T5Kcc13OuSbR\nL35LkJRiZgmSUiUdE33iOefca5JOnXd5WP1gZsWSMp1za110R+H/P+A9oyZooatU0pEBz4/GrsFj\nZjZJ0kJJ6yQVOedqYy8dl1QUe0x/eePfJP13SX0DrtEn/posqV7SY7Fp30fNLE30i2+cczWS/lXS\nYUm1kpqdc78SfRIvhtsPpbHH518fVUELXYgDZpYu6aeSHnTOtQx8LfZ/HJxj4hEze6+kOufcxndq\nQ5/4IkHSVZK+45xbKKldsemSs+gXb8XWCC1XNBCXSEozs08ObEOfxId47oegha4aSeUDnpfFrsEj\nZhZRNHD90Dn3XOzyidhQr2Lf62LX6a/Rd62k95vZQUWn2282sydFn/jtqKSjzrl1sefPKhrC6Bf/\n3CrpgHOu3jnXLek5SdeIPokXw+2Hmtjj86+PqqCFrg2SppvZZDNLVHRx3c99rikwYjtDviep2jn3\njQEv/VzSp2OPPy3pPwdc/5iZJZnZZEUXOq73qt4gcM79tXOuzDk3SdG/D6845z4p+sRXzrnjko6Y\n2RWxS7dIekv0i58OS1pqZqmxf8tuUXRdKn0SH4bVD7GpyBYzWxrrz08NeM+oSRjtHxBPnHM9ZvZn\nkn6p6M6T7zvndvhcVpBcK+luSdvMbHPs2t9I+idJz5jZvZIOSfqIJDnndpjZM4r+sumR9DnnXK/3\nZQcSfeK/z0v6Yex/EPdLukfR/1GmX3zgnFtnZs9K2qTon/Gbit5eJl30iafM7ClJN0rKN7Ojkr6q\nS/s367OK7oRMkfRi7Gt0a+c2QAAAAKMvaNOLAAAAviB0AQAAeIDQBQAA4AFCFwAAgAcIXQAAAB4g\ndAEAAHiA0AUAAOCB/wtEEbBOiZEH0AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efeb32a0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
